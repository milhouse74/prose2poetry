%

% File final_report.tex
%
%% Based on the style files for ACL 2020, which were
%% Based on the style files for ACL 2018, NAACL 2018/19, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2020}
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{fancyvrb}
\renewcommand{\UrlFont}{\ttfamily\small}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}
\usepackage[T1]{fontenc}

\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B\textsc{ib}\TeX}

\newenvironment{tight_enumerate}{
\begin{enumerate}
\setlength{\itemsep}{0pt}
\setlength{\parskip}{0pt}
}{\end{enumerate}}

\newenvironment{tight_itemize}{
\begin{itemize}
\setlength{\itemsep}{0pt}
\setlength{\parskip}{0pt}
}{\end{itemize}}

\title{Prose2poetry -- generating rhyming couplets with Natural Language Processing}

\author{Sophie Bulman \\
  Affiliation / Address line 1 \\
  \texttt{email@domain} \\\And
  Sevag Hanssian \\
  McGill University \\
  \texttt{sevag.hanssian@mail.mcgill.ca} \\\AND
  François Milot \\
  Université de Montréal \\
  \texttt{francois.milot@gmail.com} \\}

\date{}

\begin{document}
\maketitle
%%%%%%%%%%%%%%
% ABSTRACT			%
%%%%%%%%%%%%%%
\begin{abstract}
	Natural language generation (NLG) is a rich subfield of natural language processing (NLP). The task of evaluating machine-generated text without human judges is complex and multi-faceted \cite{nlgeval}. Poetry in particular is difficult to judge due to its characteristic metaphor and subjective nature. A very simple form of poetry is a 2-sentence couplet with end rhymes. In this paper, a model is proposed that can generate simple rhyming couplets, with only non-poetic (i.e. prose) texts as an input. The model's outputs compare favorably to couplets written by humans, according to a proposed quantitative couplet scoring function.
\end{abstract}

%%%%%%%%%%%%%%
% INTRODUCTION		%
%%%%%%%%%%%%%%
\section{Introduction}
\label{sec:intro}

Poetry is a form of writing that condenses emotions, stories, and thoughts into rhyming verses. Devices such as rhyme and meter are used to impart musicality to the written lines, transforming them into an artistic performance beyond simple information transfer. Underneath the surface, or \textit{form} of the poem, \textit{metaphor} is often used to weave subtexts and hidden meanings. There are many style of poems ranging from the \textit{haiku} and \textit{sonnet}, which have strict rules, to \textit{free verse} which has a more flexible structure \citep{poem_type}.

Rhyme is not the only component of poetry, but it is one of the most important. In rhyming poems, rhymes can occur in the middle of the lines, known as \textit{internal} or \textit{middle rhyme} \cite{internal_rhyme_def}, but the most typical is the \textit{end rhyme} \cite{end_rhyme_def} on the last words of the lines. There are also multiple types of rhyme. There is \textit{perfect rhyme}, where the two words have the exact same assonance and number of syllables, \textit{masculine rhyme}, where the rhyme is on the stressed last syllable, and \textit{feminine rhyme}, where the rhyme is on the unstressed last syllable \citep{poem_rhyme_type}.

A simple form of poetry is the \textit{couplet}, which is a pair of sentences forming a unit that rhymes \cite{couplet_def} For simplicity throughout this paper, any two-line pair where the last words rhyme -- \textit{end rhymes}, as mentioned previously -- will be considered a valid couplet. This ignores metaphor and other subjective aspects of poetry (that are hard to define and measure), and also ignores the more complex rhyming structures. A simple couplet example is provided in figure \ref{fig:couplet_example}.

\begin{figure}
	\textit{The farmer milked the goat,} \newline
	\textit{He also owned a boat}
\caption{Example of a simple couplet with end rhymes}
\label{fig:couplet_example}
\end{figure}

The model proposed by this paper, named \textit{prose2poetry}, will take a non-rhyming prose corpus (e.g. an English novel) and a seed word as inputs. As its output, it will generate rhyming couplets based on the theme of the seed word, built from the vocabulary and language of the input corpus. This is split into two major components:
\begin{tight_enumerate}
	\vspace{-0.5em}
	\item
		Generate pairs of rhyming words from the input corpus vocabulary that are semantically related to the seed word.
	\item
		Generate two-sentence pairs from the prose corpus in a backwards direction, by setting the end words to be the rhyming word pairs from the previous step.
\end{tight_enumerate}
\vspace{-0.5em}

To judge the success of the model, a quantitative couplet score is proposed, which in addition to rhymes considers semantic relatedness and NLG evaluation scores to penalize nonsensical outputs. The scores of the generated couplets will be compared to scores of human-written couplets in select couplet corpora of varying quality. This is realized using a gold standard corpus of published poetry written by historically significant poets, and a silver standard corpus of unpublished and uncurated crowd-sourced human poetry.

%%%%%%%%%%%%%%
% RELATED WORK		%
%%%%%%%%%%%%%%
\section{Related work}
\label{sec:related}

The task of generating poetry has been explored by \citet{cole}, where the model generated rhyming couplets using neural networks trained on rhyming couplets as an input. The author introduced a useful technique to filter the Gutenberg poetry dataset \cite{gutenbergpoetry} into a corpus of gold-standard rhyming couplets, used as an input to a neural model. \citet{hopkins-kiela-2017} also introduced neural generator models trained on poetry as inputs using different embeddings. \citet{Xie2017DeepP} proposed a deep learning system that learns the form of input Shakespeare sonnets, and produces outputs in the same style. \textit{prose2poetry} differs from these approaches by explicitly not using rhyming poetry as inputs, and instead using heuristic rules to shape prose into poetry.

A paper by \citet{keswarani} describes several techniques for NLP-driven quantitative measures of rhyme and poem scoring for poetry classification, evaluated on datasets from PoetryFoundation, an open platform for user-submitted poetry \cite*{poetryfoundation}.

A simplifying decision in this paper, mentioned in section \ref{sec:intro}, was to ignore the artistic and metaphorical aspects of poetry and focus only on the form. \citet{bena2020introducing} criticize this approach, claiming that ``forcing a model to adhere to specific rules or templates, or summarizing or translating a given text to generate new poetry is unlikely to lead to the artistically expressive quality we seek to generate.'' They proposed to fine-tune the pre-trained language model GPT-2 with creative elements through emotion classification. This is a valid criticism of simpler approaches, but beyond the scope of \textit{prose2poetry}.

%%%%%%%%%%%%%%
% METHOD			%
%%%%%%%%%%%%%%
\section{Method}
\label{sec:method}

\subsection{Input corpus}

The input corpora tested were free and public domain English novels from the nltk Gutenberg dataset \cite[Chapter~2]{gutenbergnltk}. These are available in a preprocessed form in nltk. Some additional preprocessing in the \textit{prose2poetry} model is to drop tokens that only consisted of punctuation, strip punctuation from words, and omit various author, title page, chapter, and volume headings.

\subsection{Generating word pairs}

\subsubsection{Rhyming dictionary}

From the related works section in \ref{sec:related}, \citet{keswarani}, \citet{cole}, and \citet{hopkins-kiela-2017} use the CMUdict phoneme dictionary \cite{cmudict}, either directly or indirectly through the alternative interface in the pronouncingpy Python library \cite{pronouncingpy}. The CMUdict provides pronunciations for words with the ARPAbet phonetic transcription codes \cite[Chapter~27]{jurafsky}.

\begin{figure}
\begin{Verbatim}[fontsize=\small]
>>> from nltk.corpus import cmudict
>>> cmudict.dict()['bow']
[['B', 'AW1'], ['B', 'OW1']]
>>>
>>> import pronouncing
>>> pronouncing.phones_for_word('bow')
['B AW1', 'B OW1']
\end{Verbatim}
\caption{CMUdict phonemes for the word ``bow''}
\label{fig:cmudict}
\end{figure}

Some simple Python code in figure \ref{fig:cmudict} shows the phoneme pronunciation information for the word ``bow'' exposed by the CMUdict, both from the CMUdict corpus in nltk and from the pronouncingpy interface.

\subsubsection{Rhyme score}
\label{sec:rhymescore}
The custom rhyme score function uses the phoneme data from the CMUdict in a weighted sum of three heuristic scores -- \textit{reverse consecutive phoneme matching} (RCPM), \textit{overall phoneme matching} (OPM), and \textit{syllable count matching} (SCM), all of which are in the range $\in [0, 1]$.
\begin{tight_itemize}
	\vspace{-0.5em}
	\item \textit{Reverse consecutive phoneme matching} (RCPM):
	RCPM counts, in reverse order starting from the last phoneme of both words, how many consecutive phoneme matches occur between the two words, normalized by the maximum possible matches. Let $c^{(a,b)}_{\text{phoneme}}$ be the reverse consecutive phoneme matches between words $a$ and $b$ and let $n^{(a)}_{\text{phoneme}}$ be the phoneme count of word $a$.
	$$\textrm{RCPM}^{(a,b)} = \frac{c^{(a,b)}_{\text{phoneme}}}{\min(n^{(a)}_{\text{phoneme}}, n^{(b)}_{\text{phoneme}})}$$
	The purpose of the reverse match score is to emphasize the importance of the last phoneme for end rhymes.
	\item \textit{Overall phoneme matching} (OPM):
		OPM counts all common phonemes between a pair of words without considering order. For words that don't have any consecutive identical reverse phonemes, they may still exhibit a smaller degree of rhyme. Let $m^{(a,b)}_{\text{phoneme}}$ be the total number of phoneme matches between word $a$ and $b$, and $n^{(a)}_{\text{phoneme}}$ be the phoneme count of word $a$.
		$$\textrm{OPM}^{(a,b)} = \frac{2 * m^{(a,b)}_{\text{phoneme}}}{n^{(a)}_{\text{phoneme}} + n^{(b)}_{\text{phoneme}}}$$
		This can be considered an adapted Ratcliff/Obserhelp string similarity score \cite{ratcliff}, using phonemes instead of characters.
	\item \textit{Syllable count matching} (SCM):
	The last score is to prefer words that have the same or similar number of syllables. Let $n^{(a)}_{\text{syllable}}$ be the total amount of syllables in word $a$.
	\begin{equation}
	\textrm{SCM}^{(a,b)} = 
	\begin{cases}
	\nonumber 1 \text{ if $\max(n^{(a)}_{\text{syllable}}, n^{(b)}_{\text{syllable}})$ = 1}\\
	\nonumber \frac{\min(n^{(a)}_{\text{syllable}}, n^{(b)}_{\text{syllable}}) - 1}{\max(n^{(a)}_{\text{syllable}}, n^{(b)}_{\text{syllable}}) - 1} \text{ otherwise}\\
        \end{cases}
	\end{equation}
	The subtraction by 1 is to ensure that the lowest possible SCM score can be 0.
\end{tight_itemize}

The weights assigned to each score were fine-tuned with manual adjustment, leading to the final weight table in \ref{table:weight_rhyme_score}.

\begin{table}[ht]
\centering
\begin{tabular}{c c c}
	\hline\hline
	RCPM & OPM & SCM \\ [0.5ex]
	\hline
	70\% & 15\% & 15\% \\ [0.5ex]
	\hline
\end{tabular}
\caption{Weights of rhyme score}
\label{table:weight_rhyme_score}
\end{table}

\begin{figure}
\begin{Verbatim}[fontsize=\small]
>>> rhyme_score('affection', 'perfection')
0.88
>>> rhyme_score('affection', 'disgust')
0.075
\end{Verbatim}
\caption{Some examples of rhyme score}
\label{fig:rhymescorecode}
\end{figure}

Some examples of rhyme scores are shown in the code example in \ref{fig:rhymescorecode}. There are two more additions for some edge cases:

\begin{tight_enumerate}
	\vspace{-0.5em}
	\item
		Identical words have their rhyme score set to 0. As a rule, a word shouldn't rhyme with itself in the generated couplets.
	\item
		Longer rhymes are preferred, up to a ceiling of 6 phonemes. Without this factor, single-syllable words dominate the rhyme score, and multi-syllable words are more interesting in a couplet.
\end{tight_enumerate}

\subsubsection{Multiple pronunciations and sense disambiguation}

A considerable problem encountered was that of associating the different pronunciations of homographs with their respective senses or parts of speech. \citet{hopkins-kiela-2017} summarize the problem as follows:

\begin{quote}
Pronunciation dictionaries have often been used to determine the syllabic stresses of words, but suffer from some limitations [...]. All word pronunciations are considered equiprobable, including archaic and uncommon pronunciations, and pronunciations are provided context free, despite the importance of context for pronunciation.
\end{quote}

\begin{figure}
\begin{Verbatim}[fontsize=\small]

>>> from nltk.corpus import wordnet, cmudict
>>> cmudict.dict()['bow']
[['B', 'AW1'], ['B', 'OW1']]
>>>
>>> wordnet.synsets('bow')
[Synset('bow.n.01'), Synset('bow.n.02'),
 Synset('bow.n.03'), Synset('bow.n.04'),
 Synset('bow.n.05'), Synset('bow.n.06'),
 Synset('bow.n.07'), Synset('bow.n.08'),
 Synset('bow.n.09'), Synset('bow.v.01'),
 Synset('submit.v.06'), Synset('bow.v.03'),
 Synset('crouch.v.01'), Synset('bow.v.05')]
>>>
>>> cmudict.dict()['defect']
[['D', 'IY1', 'F', 'EH0', 'K', 'T'],
 ['D', 'IH0', 'F', 'EH1', 'K', 'T']]
>>>
>>> wordnet.synsets('defect')
[Synset('defect.n.01'),
 Synset('defect.n.02'),
 Synset('defect.n.03'),
 Synset('blemish.n.01'),
 Synset('defect.v.01')]
\end{Verbatim}
\caption{Examples of independent pronunciation and word sense ambiguity}
\label{fig:wordnet}
\end{figure}

Consider two illustrating examples:
\begin{tight_enumerate}
	\vspace{-0.5em}
	\item
		``bow'' can be pronounced like ``cow'', or like ``snow''.
	\item
		``defect'' can be pronounced with a beginning emphasis, i.e. ``DEE-fect'', the noun, or with an end emphasis, ``de-FECT'', the verb.
\end{tight_enumerate}

These are complex ambiguities, illustrated alongside their WordNet synsets and parts of speech in figure \ref{fig:wordnet}. This can only be resolved with a lexical resource that provides combined sense and pronunciation disambiguation. One attempted solution was to penalize words with multiple pronunciations by dividing the rhyme score by the total number of pronunciation permutations. However this led to the rhyme score being unable to find any good rhymes. \citet{keswarani} ignored senses for simplicity and took the maximum rhyme score of all the permutations of the different pronunciations of word $a$ and word $b$ -- the same approach was followed in this paper.

\subsubsection{Word embedding}
\label{sec:fasttext}
The next step is to ensure that the two rhyming words are also semantically or contextually related. This is to make the job of the sentence generator easier in the subsequent module easier. Creating a couplet consisting of two semantically related sentences is the major goal of this paper, and it's assumed that it will be easier to generate these from semantically related end words.

The approach taken to quantify the context, or semantic meaning, of the two words is the FastText word embedding \cite{fasttext}. This embedding is similar to word2vec \cite{wordvec} but, in particular, handles out-of-vocabulary better.

The word embedding is trained on the input prose corpus. This ensures that the vocabulary of the generated rhyming word pairs come from the English novel that was selected as the input. The hyperparameters selected are shown in table \ref{table:HP_fasttext}.

\begin{table}[ht]
\centering
\begin{tabular}{ll c c}
	\hline\hline
	Hyperparameter & Value \\ [0.5ex]
	\hline
	Vector size & 128 \\ [0.5ex]
	Window size & 32 \\ [0.5ex]
	Min count & 5 \\ [0.5ex]
	Sample & 0.01\\ [0.5ex]
	Skip-gram & True\\ [0.5ex]
	Epochs  & 50\\ [0.5ex]
	\hline
\end{tabular}
\caption{Hyperparameters of FastText}
\label{table:HP_fasttext}
\end{table}

\subsubsection{Word pair creation}
\begin{tight_enumerate}
	\item \textit{Theme selection}: To be able to generate a pair of words, the system needs to have a \textit{seed word}. This can be considered the ``theme'' of the couplets that will be generated.
	\item \textit{Gather context-neigbors of theme}: From the seed word, a list of the most similar words in context is generated. More specifically, the FastText model exposes a cosine distance measure to establish the neighbors of that word. From that list, a list of permutations of all possible word pairs that are near the seed word in context can be created.
	\item \textit{Word-pair scoring}: Given a list of all the possible word pairs, each pair can be scored with their respective semantic distance to the theme and with their rhyme score (see section \ref{sec:rhymescore}). The weights assigned to the semantic and rhyme scores were selected to emphasize rhyming over context. The selected weights are shown in table \ref{table:weight_wordpair}. The top-scoring pairs in the final list are used as inputs to the sentence generation.
\begin{table}[ht]
\centering
\begin{tabular}{c c}
	\hline\hline
	Rhyme score & FastText distance\\ [0.5ex]
	\hline
	80\% & 20\% \\ [0.5ex]
	\hline
\end{tabular}
\caption{Weights in word pair scoring}
\label{table:weight_wordpair}
\end{table}
\end{tight_enumerate}

\textcolor{red}{
\subsection{Sentence generation}
\label{sec:languagegen}
The next step is to feed high-scoring rhyming pairs of words to a language generator model, by setting each rhyming word as the last word in 2 sentences. The generator model will use the same prose corpus as its only input. It should then generate two sentences that have a rhyming last word, which is in essence a very basic rhyming couplet.
LSTM, Markov, etc.
\textbf{how about a very naive model that tries to find existing sentences that end with rhyming word pairs and just join them}
Since we pick rhyming \textit{last word} pairs, we do our generation back-to-front. Specific challenges?
}


%%%%%%%%%%%%%%
% EXP & RESULTS		%
%%%%%%%%%%%%%%
\section{Experiments and Results}
\label{sec:results}

\subsection{Baseline corpora}
\label{sec:corpora}

Using a similar approach to \citet{cole}, the Gutenberg Poetry \cite{gutenbergpoetry} and PoetryFoundation \cite{poetryfoundationkaggle} datasets were filtered to produce rhyming couplets, as its easier to extract couplets from larger poetry datasets than it is to find datasets exclusively containing couplets.

The filtering process uses the \verb|rhymes()| function of the pronouncingpy library to extract every pair with rhyming end words from the same poem as a couplet. The use of an external rhyme function (and not the rhyme score from section \ref{sec:rhymescore}) is to avoid bias.

The total is 331,713 rhyming couplets from Gutenberg, and 5,909 rhyming couplets from PoetryFoundation. Some examples can be seen in figure \ref{fig:couplet_examples}. These datasets are used as baselines, where Gutenberg is considered the gold standard, since they come from a body of published and famous authors, and PoetryFoundation is considered the silver standard, since they can be submitted by anybody.

\begin{figure}
	\textit{That lies between thine eyelids and thine eyes,} \newline
	\textit{Like a flower laid upon a flower it lies} \newline
	-- Gutenberg
	\vspace{0.5em}
	\newline
	\textit{How we on our own tickle the chin,} \newline
	\textit{Of the prince or the dame that lets us in} \newline
	-- PoetryFoundation
\caption{An example of anonymized couplets from Gutenberg and PoetryFoundation}
\label{fig:couplet_examples}
\end{figure}

\subsection{Couplet scoring}
\label{sec:coupletscore}
To score a couplet, a weighted sum of multiple metrics was used to compute a single scalar value $\in [0, 1]$:
\begin{tight_itemize}
	\vspace{-0.5em}
	\item \textit{Rhyme score}: This is the measure that was introduced in section \ref{sec:rhymescore}, applied to the two end words of the two sentences of the couplet.
	\item \textit{Stress}: Using the pronouncingpy library, the syllabic stress of all of the words in each line was concatenated. 0 represents unstressed, 1 represents primary stress, and 2 represents secondary stress as defined by the CMUdict. Then, a string similarity score \cite{ratcliff} was applied. This should score high for sentences that are stressed in the same way, selecting for \textit{meter} in the couplets.
	\item \textit{Semantic coherence}: This metric is ensure both lines of the couplet are close to each other in context, or semantically. For this, the doc2vec embedding model \cite{docvec} was used. The training of this model was similar to the FastText embedding (see table \ref{table:HP_fasttext}) but with larger window (size 64) and using distributed memory for the training method.
	\item \textit{NLG evaluation score}: Using the conclusions of \citet{nlgeval}, the NLG score METEOR \cite{meteor} was selected. \citet{nlgeval} found that METEOR mapped reasonably well to human judgement, and importantly also judged sentence quality on their own, regardless of the reference sentences. The downside of METEOR (and any NLG metric) is that it requires the inputs, or the lines of the couplet in this case, to be compared to a list of \textit{reference sentences}. The Gutenberg gold standard baseline was selected to be the reference, to ensure that sentences that resemble real poetry score highly in METEOR.

		It would be unfeasible due to computational cost to use all 331,713 Gutenberg couplets as an input, so a limited subset had to be selected. A random selection of 5,000 couplets was used as the reference. Finally, several evaluation runs were done with different RNG seeds, and the total METEOR score was relatively stable across all runs. This concluded that randomization didn't influence the scores strongly.
\end{tight_itemize}

The weights assigned to each score were again fine-tuned with manual adjustment, leading to the final weight table in \ref{table:weight_couplet_score}.

\begin{table}[ht]
\centering
\begin{tabular}{c c c c}
	\hline\hline
	Rhyme & Stress & Semantic & NLG \\ [0.5ex]
	\hline
	70\% & 10\% & 10\% & 10\% \\ [0.5ex]
	\hline
\end{tabular}
\caption{Weights of couplet score}
\label{table:weight_couplet_score}
\end{table}

\textcolor{blue}{
\subsection{Results}
\label{sec:results}
\subsubsection{Quantitative assessment}
Show the stats here of rhyme score of 1000 random couplets from each of the 2 good poetry baselines, 1 bad prose baseline, our custom models
\subsubsection{Qualitative assessment}
Put good generated poem and bad generated poem
}

%%%%%%%%%%%%%%
% DISC & CONC		%
%%%%%%%%%%%%%%
\textcolor{green}{
\section{Discussion and conclusion}
\label{sec:discconc}
fmilot: I can help on this one once the results are putted there.
}


%%%%%%%%%%%%%%
% STAT OF CONT		%
%%%%%%%%%%%%%%
\section{Statement of contributions}
\label{sec:contributions}
All members equally participated in the project. Also, the design and direction of the project have been decided as a group. 

Sophie Bullman worked on the backward sentence generation. François Milot developed the word embedding (FastText), the doc2vec training, the word pair generation and design of the rhyme score. Sevag Hanssian worked on finding different poem baselines \& the prose dataset, creating the coding architecture, developing the couplet scoring and running the experiments.

\bibliographystyle{acl_natbib}
\bibliography{final_report}



\end{document}
